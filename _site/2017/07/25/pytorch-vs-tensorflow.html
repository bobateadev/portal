<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#943526">

	
	<meta property="og:image" content="http://localhost:4000/thumbnail.jpg">
	

	
	<meta property="og:title" content="Paddle Blog • Pytorch 对比 TensorFlow 使用体验的优点">
	

	<link rel="icon" href="/favicon.png" type="image/png">

	<title>Pytorch 对比 TensorFlow 使用体验的优点</title>
	<meta name="description" content="这里只讨论PyTorch相较于其他框架，值得学习的优点。">

	<link rel='stylesheet' id='libretto-fonts-css'  href='https://fonts.googleapis.com/css?family=Libre+Baskerville%3A400%2C700%2C400italic%7CPlayfair+Display%3A400%2C700%2C400italic%2C700italic%7CPlayfair+Display+SC%3A700%2C700italic%7CMontserrat%3A400%7CDroid+Sans+Mono%3A400&#038;subset=latin%2Clatin-ext' type='text/css' media='all'>

	<link rel="stylesheet" href="/css/main.css">
	<link rel="canonical" href="http://localhost:4000/2017/07/25/pytorch-vs-tensorflow.html">
	<link rel="alternate" type="application/rss+xml" title="Paddle Blog" href="http://localhost:4000/feed.xml">
</head>


<body>

	<header class="site-header">

	<div class="wrapper">

		<a class="site-title" href="/">Paddle Blog</a>

	</div>

</header>


	<div class="page-content">
		<div class="wrapper">
			<header class="post-header">
	<div>
		<span>Posted on </span><span class="post-meta">July 25, 2017</span>
	</div>

	<h1 class="post-title" itemprop="name headline">Pytorch 对比 TensorFlow 使用体验的优点</h1>
</header>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="entry-content" itemprop="articleBody">
		<p>这里只讨论PyTorch相较于其他框架，值得学习的优点。</p>

<h1 id="几乎原生的python使用体验">几乎原生的python使用体验</h1>
<p>直接构建自 Python C API，从细粒度上直接支持python的访问。</p>

<p>带来的优势：</p>

<ul>
  <li>完全 python 化的使用体验，降低 pythoner 适应的门槛</li>
  <li>可以直接用原生python语法定义新的 operation</li>
</ul>

<p>这些方面，tensorflow 完全是另外一种体验：</p>

<ul>
  <li>总有一种用 python 调用 C++ 写的第三方动态链接库的感觉</li>
  <li>写模型需要更多代码，无法贯彻 python 简约风格</li>
  <li>新的 operation 必须用 C++ 开发</li>
</ul>

<p>下面演示 PyTorch 与 python 结合的紧密程度[1]：</p>

<p>比如用 numpy 实现一个 2 层的神经网络：</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># N is batch size; D_in is input dimension;</span>
<span class="c"># H is hidden dimension; D_out is output dimension.</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c"># Create random input and output data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="c"># Randomly initialize weights</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
  <span class="c"># Forward pass: compute predicted y</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
  <span class="n">h_relu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
  
  <span class="c"># Compute and print loss</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
  
  <span class="c"># Backprop to compute gradients of w1 and w2 with respect to loss</span>
  <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
  <span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
  <span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_h_relu</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
  <span class="n">grad_h</span><span class="p">[</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_h</span><span class="p">)</span>
 
  <span class="c"># Update weights</span>
  <span class="n">w1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
  <span class="n">w2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>
</code></pre>
</div>
<p>相同的功能使用 PyTorch 实现</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span>
<span class="c"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span>

<span class="c"># N is batch size; D_in is input dimension;</span>
<span class="c"># H is hidden dimension; D_out is output dimension.</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c"># Create random input and output data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="c"># Randomly initialize weights</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
  <span class="c"># Forward pass: compute predicted y</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
  <span class="n">h_relu</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>

  <span class="c"># Compute and print loss</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

  <span class="c"># Backprop to compute gradients of w1 and w2 with respect to loss</span>
  <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
  <span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
  <span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_h_relu</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
  <span class="n">grad_h</span><span class="p">[</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_h</span><span class="p">)</span>

  <span class="c"># Update weights using gradient descent</span>
  <span class="n">w1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
  <span class="n">w2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>
</code></pre>
</div>
<p>除去几个激活函数，<strong>PyTorch 在实现中几乎只引入了一个 <code class="highlighter-rouge">FloatTensor</code> 的概念，主要的代码结构和原生的 python 实现基本一致。</strong></p>

<p>再看看 tensorflow 的情况：</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c"># First we set up the computational graph:</span>

<span class="c"># N is batch size; D_in is input dimension;</span>
<span class="c"># H is hidden dimension; D_out is output dimension.</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c"># Create placeholders for the input and target data; these will be filled</span>
<span class="c"># with real data when we execute the graph.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">D_out</span><span class="p">))</span>

<span class="c"># Create Variables for the weights and initialize them with random data.</span>
<span class="c"># A TensorFlow Variable persists its value across executions of the graph.</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">((</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)))</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">((</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)))</span>

<span class="c"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span>
<span class="c"># Note that this code does not actually perform any numeric operations; it</span>
<span class="c"># merely sets up the computational graph that we will later execute.</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
<span class="n">h_relu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_relu</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

<span class="c"># Compute loss using operations on TensorFlow Tensors</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c"># Compute gradient of the loss with respect to w1 and w2.</span>
<span class="n">grad_w1</span><span class="p">,</span> <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">])</span>

<span class="c"># Update the weights using gradient descent. To actually update the weights</span>
<span class="c"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span>
<span class="c"># in TensorFlow the the act of updating the value of the weights is part of</span>
<span class="c"># the computational graph; in PyTorch this happens outside the computational</span>
<span class="c"># graph.</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">new_w1</span> <span class="o">=</span> <span class="n">w1</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">w1</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span><span class="p">)</span>
<span class="n">new_w2</span> <span class="o">=</span> <span class="n">w2</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">w2</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span><span class="p">)</span>

<span class="c"># Now we have built our computational graph, so we enter a TensorFlow session to</span>
<span class="c"># actually execute the graph.</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="c"># Run the graph once to initialize the Variables w1 and w2.</span>
  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

  <span class="c"># Create numpy arrays holding the actual data for the inputs x and targets y</span>
  <span class="n">x_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span>
  <span class="n">y_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c"># Execute the graph many times. Each time it executes we want to bind</span>
    <span class="c"># x_value to x and y_value to y, specified with the feed_dict argument.</span>
    <span class="c"># Each time we execute the graph we want to compute the values for loss,</span>
    <span class="c"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span>
    <span class="c"># arrays.</span>
    <span class="n">loss_value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">new_w1</span><span class="p">,</span> <span class="n">new_w2</span><span class="p">],</span>
                                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">x_value</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_value</span><span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="n">loss_value</span><span class="p">)</span>
</code></pre>
</div>
<p>直观感觉 tensorflow 的实现要复杂一些：</p>

<ul>
  <li>引入了 <code class="highlighter-rouge">placeholder</code>, <code class="highlighter-rouge">Variable</code>, <code class="highlighter-rouge">tf.Session</code>, <code class="highlighter-rouge">feed_dict</code> 等新概念
    <ul>
      <li>新手可能不清楚 <code class="highlighter-rouge">placeholder</code> 和 <code class="highlighter-rouge">Variable</code> 的概念
        <ul>
          <li>进而要去了解 tensorflow 计算图模型，实现细节等</li>
          <li>有点反小白</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>代码多了很多，相当一部分跟原生代码的结构不太一致</li>
</ul>

<p><b><font color="red">用 python 扩展 PyTorch 的 operaion 也很很简单</font></b>：</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="k">class</span> <span class="nc">MyReLU</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
  <span class="s">"""
  We can implement our own custom autograd Functions by subclassing
  torch.autograd.Function and implementing the forward and backward passes
  which operate on Tensors.
  """</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="s">"""
    In the forward pass we receive a Tensor containing the input and return a
    Tensor containing the output. You can cache arbitrary Tensors for use in the
    backward pass using the save_for_backward method.
    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="s">"""
    In the backward pass we receive a Tensor containing the gradient of the loss
    with respect to the output, and we need to compute the gradient of the loss
    with respect to the input.
    """</span>
    <span class="nb">input</span><span class="p">,</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
    <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">grad_input</span><span class="p">[</span><span class="nb">input</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">grad_input</span>
</code></pre>
</div>

<p>综合上面原生python，PyTorch, tensorflow 对同一个模型实现的比较可以看出 PyTorch 的优势在以下几点：</p>

<ul>
  <li><strong>相比于原生的实现，引入的新概念很少，降低了 python 用户理解的门槛</strong></li>
  <li>代码基本跟原生的 python 实现一致，统一的 python 实现的思维</li>
  <li><strong>可以直接用原生 python 代码扩展 PyTorch 的 operation</strong></li>
</ul>

<h1 id="基于-tensor-的扩展能力">基于 tensor 的扩展能力</h1>
<p>这一点上 PyTorch 和 Tensorflow 类似，可以用 build-in 的 operation 去组合出新的大粒度的 operation；而对比 paddle/Caffe 粗粒度的layer 支持， PyTorch/Tensorflow 能真正实现NN编程，而前者更大程度上是在配置NN。</p>
<h1 id="api封装设计合理与kerastensorflow对比">API封装设计合理（与Keras/tensorflow对比）</h1>
<p>这里我们只讨论 PyTorch 的原生API，不同于 Tensorflow 有 Keras, tf.contrib, tf.learn 等多套官方API，<strong>PyTorch 官方只有一套 API，但良好的上手体验并不逊色于 Tensorflow 的几套 API。</strong></p>

<p>PyTorch的API有点类似 Keras 和 Tensorflow 裸API的结合：</p>

<ul>
  <li>常用的 NN 模块封装上类似 Keras 的 functional API，layer作为 function 对不同的 input 输出包装好的 output tensor
    <ul>
      <li>比如 <code class="highlighter-rouge">conv1 = nn.Conv2d(1 10, kernal_size=5)</code></li>
    </ul>
  </li>
  <li>除了对常用 NN 模块的封装，其他API没有做太多封装/修饰，保持底层和简洁（用起来类似 Tensorflow 裸 API）
    <ul>
      <li>Keras 中很多封装非常高层，但用户无法避免依赖 Tensorflow 的原生接口
        <ul>
          <li>比如 <code class="highlighter-rouge">Model</code> 接口基本隐藏了底层 Tensorflow 的所有细节，但大多数情况，类似 <code class="highlighter-rouge">tf.scope</code>, <code class="highlighter-rouge">tf.device</code> 等原生接口的使用是无法避免的，这就需要两种层次的接口混用</li>
          <li>Keras 和 Tensorflow 原生API的风格不太一致，增加了用户在两者切换/混用的学习门槛</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>总之，PyTorch的API设计上，粒度适中，并且官方的接口比较统一，方便用户持续积累好的代码模式；相比较 TensorFlow 则有多套接口，且抽象的程度各不相同，可能不利于用户经验的积累。</p>

<h1 id="dynamic-net相关">Dynamic net相关</h1>
<p>得益于直接基于 python C API 构建的 python 接口，PyTorch 支持动态网络的构建；不同于 Tensorflow 在运行前需要生成静态计算图，PyTorch的程序可以在执行时动态构建/调整计算图。</p>

<p>比如 TreeLSTM<a href="#参考文献">[2</a>] 模型将 LSTM 构建在一个树结构的网络上，具体应用比如，以语法树为形状构建神经网络，用来学习句子信息；每个句子都有不同的语法树，因此需要构建不同的神经网络（树各个节点的模型参数共享，但连接关系会动态改变）。</p>

<p>这个模型 PyTorch 和 TensorFlow 都有支持，前者利用了自己支持 dynet 的优势，实现起来比较自然；后者通过<code class="highlighter-rouge">tf.while_loop</code> 等条件分支 operation 支持，需要对原始逻辑进一步抽象才能支持。</p>

<p>其中， PyTorch 的实现直接使用了递归的方式遍历树来构建网络（numpy 怎么实现，PyTorch就可以）[4]</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">expr_for_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">decorate</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tree</span><span class="o">.</span><span class="n">isleaf</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">makevar</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w2i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">assert</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isleaf</span><span class="p">())</span>
      <span class="n">expr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_for_tree</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">decorate</span><span class="p">:</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">_e</span> <span class="o">=</span> <span class="n">expr</span>
      <span class="k">return</span> <span class="n">expr</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">),</span> <span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">e1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_for_tree</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">decorate</span><span class="p">)</span>
    <span class="n">e2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_for_tree</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decorate</span><span class="p">)</span>
    <span class="n">expr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">TANH</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">WR</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">e1</span><span class="p">,</span> <span class="n">e2</span><span class="p">),</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">decorate</span><span class="p">:</span>
      <span class="n">tree</span><span class="o">.</span><span class="n">_e</span> <span class="o">=</span> <span class="n">expr</span>
    <span class="k">return</span> <span class="n">expr</span>
</code></pre>
</div>

<p>Tensorflow 无法支持上面递归编译树的方式，只能把树的遍历加工成类似 map() 的操作[5]，作者脑洞很大啊。</p>

<p>大体上，把二叉树用一个 <code class="highlighter-rouge">2XN</code> 的数组表示，比如</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[ 
[-1, 2, 3],
[3, 1, 0]]
</code></pre>
</div>
<p>其中 <code class="highlighter-rouge">N</code> 表示树的节点数（需要把所有的树塞进同一个tensor中，因此shape必须取最大的），其中每一列的两个数字表示当前节点左右孩子的节点 id，排列按照从叶子节点往根节点的遍历次序。</p>

<p>如此，可以将这个数组放进类似 map 的模块中，部分代码如下：</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">_recurrence</span><span class="p">(</span><span class="n">node_h</span><span class="p">,</span><span class="n">node_c</span><span class="p">,</span><span class="n">idx_var</span><span class="p">):</span>
                <span class="n">node_info</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">treestr</span><span class="p">,</span><span class="n">idx_var</span><span class="p">)</span>

                <span class="n">child_h</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">node_h</span><span class="p">,</span><span class="n">node_info</span><span class="p">)</span>
                <span class="n">child_c</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">node_c</span><span class="p">,</span><span class="n">node_info</span><span class="p">)</span>

                <span class="n">flat_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">child_h</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">tmp</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">flat_</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">cW</span><span class="p">)</span>
                <span class="n">u</span><span class="p">,</span><span class="n">o</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">fl</span><span class="p">,</span><span class="n">fr</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">tmp</span><span class="p">)</span>

                <span class="n">i</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">bi</span><span class="p">)</span>
                <span class="n">o</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="o">+</span><span class="n">bo</span><span class="p">)</span>
                <span class="n">u</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">u</span><span class="o">+</span><span class="n">bu</span><span class="p">)</span>
                <span class="n">fl</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">fl</span><span class="o">+</span><span class="n">bf</span><span class="p">)</span>
                <span class="n">fr</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">fr</span><span class="o">+</span><span class="n">bf</span><span class="p">)</span>

                <span class="n">f</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,[</span><span class="n">fl</span><span class="p">,</span><span class="n">fr</span><span class="p">])</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">u</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">f</span><span class="o">*</span><span class="n">child_c</span><span class="p">,[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

                <span class="n">node_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,[</span><span class="n">node_h</span><span class="p">,</span><span class="n">h</span><span class="p">])</span>

                <span class="n">node_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,[</span><span class="n">node_c</span><span class="p">,</span><span class="n">c</span><span class="p">])</span>

                <span class="n">idx_var</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">idx_var</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">node_h</span><span class="p">,</span><span class="n">node_c</span><span class="p">,</span><span class="n">idx_var</span>
            <span class="n">loop_cond</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a1</span><span class="p">,</span><span class="n">b1</span><span class="p">,</span><span class="n">idx_var</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">idx_var</span><span class="p">,</span><span class="n">n_inodes</span><span class="p">)</span>

            <span class="n">loop_vars</span><span class="o">=</span><span class="p">[</span><span class="n">node_h</span><span class="p">,</span><span class="n">node_c</span><span class="p">,</span><span class="n">idx_var</span><span class="p">]</span>
            <span class="n">node_h</span><span class="p">,</span><span class="n">node_c</span><span class="p">,</span><span class="n">idx_var</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="n">loop_cond</span><span class="p">,</span> <span class="n">_recurrence</span><span class="p">,</span>
                                                <span class="n">loop_vars</span><span class="p">,</span><span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre>
</div>
<p>上面实现中，对问题本身的抽象比较难，之后会用到 <code class="highlighter-rouge">tf.while_loop</code> 来支持类似 map 的扫描。</p>

<p>相比较，<strong>对于动态网络的问题，PyTorch 从任务抽象到具体实现都要简单自然很多</strong>。</p>

<h1 id="参考文献">参考文献</h1>
<ol>
  <li>http://icode.baidu.com/repo/baidu%2Fgtm%2Fgtmlib2/files/master/tree/</li>
  <li>Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks</li>
  <li>https://www.tensorflow.org/api_docs/python/tf/while_loop</li>
  <li><a href="https://gist.github.com/wolet/1b49c03968b2c83897a4a15c78980b18">PyTorch implementation for TreeLSTM</a></li>
  <li><a href="https://gist.github.com/wolet/1b49c03968b2c83897a4a15c78980b18">Tensorflow implementation for TreeLSTM</a></li>
</ol>

	</div>
</article>

<div id="post-nav">
	<div class="next">
		
	</div>

	<div class="previous">
		
	</div>
</div>

		</div>
	</div>

	<footer class="site-footer">

	<div class="wrapper">

		<h2 class="footer-heading">Paddle Blog • <a href="/table-of-contents">Table of Contents</a></h2>

		<div class="footer-col-wrapper">
			<div class="footer-col footer-col-1">
				<ul class="contact-list">
					<li>Paddle Blog</li>
					<li><a href="mailto:your-email@example.com">your-email@example.com</a></li>
				</ul>
			</div>

			<div class="footer-col footer-col-2">
				<ul class="social-media-list">
					
					<li>
						<a href="https://github.com/jekyll"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jekyll</span></a>

					</li>
					

					

					
					<li>
						<a href="https://twitter.com/jekyllrb"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">jekyllrb</span></a>

					</li>
					
				</ul>
			</div>

			<div class="footer-col footer-col-3">
				<p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
			</div>
		</div>

	</div>

</footer>


</body>

</html>
